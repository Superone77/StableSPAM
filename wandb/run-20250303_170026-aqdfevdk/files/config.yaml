_wandb:
    value:
        cli_version: 0.18.5
        m: []
        python_version: 3.11.10
        t:
            "1":
                - 1
                - 5
                - 11
                - 41
                - 49
                - 51
                - 53
                - 55
                - 71
            "2":
                - 1
                - 5
                - 11
                - 41
                - 49
                - 51
                - 53
                - 55
                - 71
            "3":
                - 3
                - 13
                - 23
                - 55
                - 61
            "4": 3.11.10
            "5": 0.18.5
            "6": 4.49.0
            "8":
                - 5
            "12": 0.18.5
            "13": linux-x86_64
act_bits:
    value: 8
act_group_size:
    value: 64
act_quant:
    value: false
act_stochastic:
    value: false
act_topk:
    value: 2
activation_checkpointing:
    value: false
batch_size:
    value: 128
beta1:
    value: 0.9
beta2:
    value: 0.999
continue_from:
    value: null
cos_threshold:
    value: 1
dataset:
    value: c4
density:
    value: 1
device:
    value: cuda:0
dtype:
    value: bfloat16
entity:
    value: null
eta:
    value: 0.5
eval_every:
    value: 1000
fp4:
    value: false
galore_scale:
    value: 1
gamma_proj:
    value: 2
gamma1:
    value: 0.85
gamma2:
    value: 0.99999
gamma3:
    value: 0.999
grad_clipping:
    value: 0
gradient_accumulation:
    value: 1
max_length:
    value: 256
max_lr:
    value: 0.0008
max_train_tokens:
    value: null
min_lr_ratio:
    value: 0.1
model:
    value:
        _attn_implementation_autoset: true
        _name_or_path: configs/llama_130m.json
        add_cross_attention: false
        architectures:
            - LLaMAForCausalLM
        attention_bias: false
        attention_dropout: 0
        bad_words_ids: null
        begin_suppress_tokens: null
        bos_token_id: 0
        chunk_size_feed_forward: 0
        cross_attention_hidden_size: null
        decoder_start_token_id: null
        diversity_penalty: 0
        do_sample: false
        early_stopping: false
        encoder_no_repeat_ngram_size: 0
        eos_token_id: 1
        exponential_decay_length_penalty: null
        finetuning_task: null
        forced_bos_token_id: null
        forced_eos_token_id: null
        head_dim: 64
        hidden_act: silu
        hidden_size: 768
        id2label:
            "0": LABEL_0
            "1": LABEL_1
        initializer_range: 0.02
        intermediate_size: 2048
        is_decoder: false
        is_encoder_decoder: false
        label2id:
            LABEL_0: 0
            LABEL_1: 1
        length_penalty: 1
        max_length: 20
        max_position_embeddings: 2048
        max_sequence_length: 1024
        min_length: 0
        mlp_bias: false
        model_type: llama
        no_repeat_ngram_size: 0
        num_attention_heads: 12
        num_beam_groups: 1
        num_beams: 1
        num_hidden_layers: 12
        num_key_value_heads: 12
        num_return_sequences: 1
        output_attentions: false
        output_hidden_states: false
        output_scores: false
        pad_token_id: -1
        prefix: null
        pretraining_tp: 1
        problem_type: null
        remove_invalid_values: false
        repetition_penalty: 1
        return_dict: true
        return_dict_in_generate: false
        rms_norm_eps: 1e-06
        rope_scaling: null
        rope_theta: 10000
        sep_token_id: null
        suppress_tokens: null
        task_specific_params: null
        temperature: 1
        tf_legacy_loss: false
        tie_encoder_decoder: false
        tie_word_embeddings: false
        tokenizer_class: null
        top_k: 50
        top_p: 1
        torch_dtype: null
        torchscript: false
        transformers_version: 4.49.0
        typical_p: 1
        use_bfloat16: false
        use_cache: true
        vocab_size: 32000
model_config:
    value: configs/llama_130m.json
name:
    value: stablespam_350_fp4_500_0.9_0.7_4e-4
num_training_steps:
    value: 20000
optimizer:
    value: stablespam
proj_bits:
    value: 8
proj_group_size:
    value: 256
proj_quant:
    value: false
proj_type:
    value: std
project:
    value: stablespam
queue_size:
    value: 5
rank:
    value: 128
restore_optimizer:
    value: true
resume_step:
    value: null
save_dir:
    value: /scratch-shared/saved
save_every:
    value: 100000
scheduler:
    value: cosine
seed:
    value: 0
simulation:
    value: false
single_gpu:
    value: false
stochastic_round:
    value: false
tags:
    value: null
total_T:
    value: 20000
total_batch_size:
    value: 512
total_params_M:
    value: 134.105856
unset_wandb:
    value: false
update_proj_gap:
    value: 1000
use_hf_model:
    value: false
warmup_steps:
    value: 2000
weight_bits:
    value: 8
weight_decay:
    value: 0
weight_group_size:
    value: 256
weight_quant:
    value: false
workers:
    value: 16
world_size:
    value: 4
